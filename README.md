# Тестовое задание Mindbox
## Задание 2

Выполненное задание находится в папках Task2 и Task2.Tests.

Юнит-тесты можно запустить командой ``dotnet test``
## Задание 3

### Ход решения:
- У нас kubernetes кластер, в котором пять нод.
Для начала создается некоторый деплоймент с шаблоном контейнера внутри(идентификатор app). А также сервис, который служит единой точкой входа для обращения к контейнерам с меткой app.
- Приложение испытывает постоянную стабильную нагрузку в течение суток без значительных колебаний. 3 пода справляются с нагрузкой.
Для этого устанавливаем количество реплик на значение 3
```yaml
spec:
  replicas: 3
```
- Размещение подов на разных нодах для отказоустойчивости.
Для этого используем политику __. Чтобы разместить поды на разных нодах, используем метку хостнейм
```yaml
    spec:
      affinity: # Настройка размещения подов на разных нодах
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector: # метка идентифицирующая приложение
              matchLabels:
                app: app
            topologyKey: "kubernetes.io/hostname"
```
- На первые запросы приложению требуется значительно больше ресурсов CPU, в дальнейшем потребление ровное в районе 0.1 CPU. По памяти всегда “ровно” в районе 128M memory.

Также здесь же необходимо установить лимиты, которые сам контейнер может потреблять, чтобы предотвратить, что контейнер будет мешать работе других контейнеров на этом ноде или произойдет неправильное масштабирование. 
Однако, в условиях не даны конкретные цифры нод, поэтому опустим лимиты.
- Приложение требует около 5-10 секунд для инициализации. Под не должен обрабатывать запросы до завершения инициализации.
Под считается готовым, если готовы все контейнеры в нем. Достаточно к контейнеру добавить пробу на готовность. Это может быть легкий эндпоинт /health или команда shell.
Проба происходит после 5 секунд от начала инициализации, каждые 5 секунд 3 раза
```yaml
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
          failureThreshold: 3
```
- Минимальное потребление ресурсов от этого deployment’а.
Можно было бы сделать автоскалирование подов, т.к. в запасе у нас есть ещё 2 свободных нода. Однако, по условию задачи, нагрузка на сервер постоянная и с ней справляются 3 пода.
Соответственно автоскалирование в данном случае не требуется.
```yaml

# Доступ к
apiVersion: v1
kind: Service
metadata:
  name: app-service
spec:
  selector:
    app: app
  ports:
  - name: http
    port: 80
    targetPort: 80

####
apiVersion: v1
kind: Deployment
metadata:
  name: app-deployment
spec:
  replicas: 3 # Количество копий подов
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
      containers:
      - image: some-container:latest
        livenessProbe:  # проверка что контейнер запущен и работает (необходим эндпоинт)
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
          failureThreshold: 3
        resources: 
          requests: # ресурсы, выделяемые на каждый запрос
            cpu: 100m
            memory: 128Mi
    spec:
      affinity: # Настройка размещения подов на разных нодах
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector: # метка идентифицирующая приложение
              matchLabels:
                app: app
            topologyKey: "kubernetes.io/hostname" # Ключ, определяющий, что для подов нужны разные хосты
```
